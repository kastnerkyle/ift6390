\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09

\title{Speech Generation with Recurrent Neural Networks}

\author{
Kyle Kastner\\
Département d’Informatique et de Recherche Opérationnelle\\
Université de Montréal, 2920 Chemin de la Tour, suite 2194,\\
Montréal (QC), H3T 1J4, Canada\\
\texttt{kastnerkyle@gmail.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
There are a wide variety of input 
representations and models used in speech
synthesis, with may research contributions from computer science, electrical
engineering, and statistics. One of the most promising models for capturing the
complex dynamics of speech is the recurrent neural network,
which holds state of the art results for the TIMIT speech recognition
task as well as many other complex time series problems. This paper will
discuss the particular concerns of applying recurrent neural networks to
perform speech generation, including datasets,
input representation, and modeling with recurrent neural networks.
\end{abstract}

\section{Outline}
The subsections below will
give a small introduction of the topics to be covered, with full coverage
reserved for the relevant section of the paper.

\subsection{Datasets}
Several datasets were used to evaluate this pipeline: speech data from the
TIMIT dataset, speech data from the LibriSpeech dataset, and onomatopeaia
sounds such as screams, moans, and whimpers from a proprietary dataset.

\subsection{Input Representations}
One of the most important parts of any machine learning pipeline is the
representation of the input. For speech synthesis, three different
representations were used: Linear Predictive Coding (LPC), sine wave
analysis, and raw input. 

\subsection{Recurrent Neural Network Architectures}
Recurrent neural networks, like feedforward networks, have a staggering
number of possible configurations and hyperparameters. This work will focus on the
specific set of hyperparamters used for sequence generation,
as well as the unique difficulties in training such an architecture.

\section{Datasets}
Finding suitable datasets is one of the most difficult (though unheralded) 
problems in machine learning. Good speech datasets are particularly difficult
to find, since most corpora of labeled speech come from 
large corporations. These collections often hold identifying information from a large number of
people and are very expensive to label, store, and curate.
\par
In this paper, the following qualifiers were used to select the datasets.
\begin{itemize}
    \item The data is well organized, with logical folder structure and naming conventions.
    \item The data is well documented, with README files or research reports detailing the collection enivronment.
    \item Examples are diverse, and span the range of conditions expected in "real world" usage. 
    \item The default resolution of examples is very high quality, and as close to "unprocessed" as possible.
\end{itemize}

Until very recently
with the release of the LibriSpeech dataset, most speech research
focused on the semi-public TIMIT dataset, with results on larger private
datasets which often change from publication to publication. In this research,
three primary datasets were used: the open and free LibriSpeech dataset,
the open but non-free TIMIT dataset, and a proprietary dataset of onomatopaeia
sounds, referenced hereafter as Ono.
\subsection{TIMIT}
The TIMIT dataset is one of the most widely cited sets in speech processing research.
Though fairly small by recent standards, the TIMIT dataset features readings from 630 different speakers, with 10 different recordings from each speaker. The raw audio is stored as 16-bit, 16000 Hz wav files. It also features time-aligned orthographic, phonetic, and word labelings for
each of the examples in the dataset. The labels and utterances have been hand verified by a number of researchers, and are considered to be the standard as far as audio and label granularity are concerned. The major downside to the TIMIT dataset is that it is non-free,
requiring a fairly substantial license fee to use. This can be prohibitive
for experimental research, or exploration by researchers whose primary focus
is not speech.
\subsection{LibriSpeech}
LibriSpeech is a new dataset published by researchers at
John Hopkins University. It is much larger than TIMIT, with approximately
1000 hours of recorded speech, with sentence level alignment and associated
text. The audio is formatted as 16-bit, 16000 Hz arff files, and is very
similar to TIMIT. It is free to use under the Creative Commons 4.0 license.
The primary drawback to using LibriSpeech is the granularty of labels and
tags, as sentence level alignment may not be sufficient for many speech
recognition tasks. For the purposes of unonditioned speech generation
this is not a problem.
\subsection{Ono}
Ono is a proprietary dataset of onomatopaeia sounds such as grunts,
yells, cries, groans and whimpers under various types of emotional
distress like pain, anger, and excitement. The audio is formatted as 16-bit,
16000 Hz wav files, and is of the same quality as TIMIT and LibriSpeech.
The label information is contained in the filename, which is largely what
category (pain, angry, etc.) and a 1-5 scale "level of emotion".
This data is fairly different from standard speech, due to the large amount
of non-harmonic sound, often called unvoiced speech, present. However, it
is very useful to show the applicability of recurrent neural networks to more
than just standard speech tasks. Unfortunately, Ono is a proprietary
dataset, and as such it will be difficult for other to reproduce these results.

\section{Input Representations}
How the data is formatted before being processed by the recurrent neural
network is crucial. Subtle differences in input representation from things
like mean centering, variance normalization, or removing correlation 
through transformations can be the difference between sucess and failure. For
the task of speech generation, there were three primary techniques
used: "raw" input (mean centered and normalized as floating point from 0-1),
sinusoidal modeling, and Linear Predictive Coding (LPC).
\par
To understand how each of these methods transforms the input, it is first
necessary to understand a bit about the particulars of recorded speech. While
deep understanding of speech is a field of study to itself, there are a few
crucial details which make speech unique, and many of the input 
transformations used will exploit these details in some way.
\par
The building blocks of speech sounds are called phonemes - these are tiny
"sub-words" that are combined to make different words. Depending on which
phoneme is spoken, a segment of speech (sometimes called a frame) is considered
to be "voiced" or "unvoiced". The primary difference between these two
categories is that voiced speech has a very rich harmonic structure,
and is fairly straight forward to model. This includes sounds such as
"ooo" and "aaah". Unvoiced speech, on the other hand, includes silence,
growls, and explosive phonemes like "puh", "tuh", and "chk". These sounds
are typically more difficult to model, due to both the rapid onset of the
sounds and the brief duration relative to the voiced component. 
\par
The Ono
dataset has a much higher density of unvoiced sounds relative to voiced than
standard speech, and this is one of the reasons we see it as a more
difficult task. The volume of speech and how it changes over time (called the
envelope) is also very important, as well as the duration and rhythm of
silences. In fact, properly modeling just the envelope and silences can
create something that is surprisingly speech-like, even if the content
itself is white-noise or as basic as a summation of a small number
of sine waves.

\subsection{Raw Input}
Raw input is a bit of a misnomer, as even in the "raw" case there are still
several things that must be done. Since the files are originally stored
as signed 16-bit values, the first step is to divide every sample by 2\^15.
This normalizes the input as floating point values in the (-1, 1) range, which
is normally small enough to avoid numerical problems. The other important
preprocessing step is to break the continuous speech into a number of
overlapping frames. The length and amount of overlap in the frame sizes can
vary, but a typical rule of thumb is 50\% overlap with frame sizes
capturing somewhere between 4 and 40 milliseconds of sound.
\par
For 16000 Hz,
this points to a frame size between 64 samples and 640 samples, with between
32 and 320 sample overlap. These numbers are chosen due to the dynamics of
speech, as each phoneme lasts (on average) about 10 milliseconds. Overlap is
crucial to providing context for the frame and relating it to what has been
seen, but there is a computational tradeoff as more overlap means
a larger computational overhead will be spent on redundant information.

\subsection{Sinusoidal Modeling}
The Discrete Fourier Transform (and the temporal version, known as the STFT)
has a long history in the realm of speech
processing. By taking the primary signal and breaking it down into a set of
sinusoids at different frequencies, we usually find the signal to be sparse in
frequency space. The transform has 
additional nice properties such as being orthogonal and linear. However, for
speech generation, a full Fourier basis is still very difficult to model, but it
is possible to reduce this set down to a few frequencies which contribute
most to making things sound
"speech-like". By estimating just 4 frequencies and their respective
gain it is possible to get a rough approximation of speech, which should be
easier to model than either the original timeseries or the full Fourier
transformed version.

\subsubsection{Linear Predictive Coding}
Linear Predictive Coding (LPC) [Atal1978] is a classic approach for robust
encoding and decoding of speech signals. It was originally designed for
use in low-bandwidth communications, but has also become the defacto
standard for reconstruction quality due to Atal’s follow up work in perceptual
audio coding. It is also the one of the compression techniques specified in
the GSM phone standard. 
\par
The physical model used for LPC comes from observing that the human vocal
system is similar to a long pipe (the vocal tract) excited by a buzzer (the
tongue), with intermediate clicks and hisses (interactions of the tongue,
teeth, and lips). To create an approximate model in this way, a long speech
signal is broken into a series of overlapping frames. Each frame is modeled by
a linear component (an FIR filter), gain, and a residual which is not modeled
by the linear filter. This coded representation of the speech can then be
compressed and transmitted, or simply transmitted directly. 
\par
There are two primary modes for using decoding LPC: pure synthesis,
and residual driven synthesis. Pure synthesis does not use the residual
component during reconstruction. Instead, it uses an additional estimation of
whether the particular frame of speech was voiced, or unvoiced. It needs
estimation of the fundamental frequency of the frame. These are related to
the actual phoneme spoken, and many methods exist to make these measurements.
A signal (Gaussian random noise in the case of unvoiced, a pulse train at the
fundamental frequency for voiced) is then run through the filter given by the
LPC coefficients. In theory, the pulse wave is supposed to approximate
the glottal pulse created by the movement of the vocal tract, but the
approximation is quite crude and results in very “robotic” speech.
\par
The second mode of decoding LPC is residual driven synthesis. In this case,
the residual excitation is used to drive the filter given by the LPC
coefficients. In the case where all of the residual is kept, the
reconstruction is nearly perfect. By compressing this residual, we have the
ability to trade reconstruction quality for feature
dimensionality reduction. We strongly prefer the residual driven synthesis
mode, as this will give the most realistic reconstruction
and we are not particularly concerned with the ”bandwidth” cost of generating
a few extra features.
\par
A few key extensions will further help us generate good LPC coefficients
from our models. The filter coefficients can be represented as Line Spectral
Frequencies (LSFs) which are much more robust to error, when compared to
the “raw” representation. This improvement will help the quality of generated
samples, by allowing the generated coefficients to have small errors
without large changes in the synthesized audio.
\section{Recurrent Architectures}
Recurrent neural networks are approximators for arbitrary dynamic systems.
This makes this type of architecture ideal for modeling data with rich
structure in both time and frequency, such as music or speech. The downside
to the raw power of recurrent neural networks is that training can be quite
difficult, and just like feedforward neural networks the hyperparamers
used for learning rate, learning rule, initialization, activation function, and
size of network have a massive impact on the performance for any specific
problem. The additional difficulty of using recurrent neural networks for
generation is that there needs to an element of randomness. The default cost
is deterministic based on the input, but we would really prefer to "seed" the
input and generate different outputs. This can be done by having the RNN learn
to predict the mean and variance parameters for a mixture of Gaussians,
rather than directly learning the output sequence. 
\par
For the recurrent model, the same architecture is used for each
representation. 
\section{Results}
\section{Conclusion}

\section*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include 
acknowledgments in the anonymized submission, only in the 
final paper. 

\section*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references. {\bf Remember that this year you can use
a ninth page as long as it contains \emph{only} cited references.}

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}
